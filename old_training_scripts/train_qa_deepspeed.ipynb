{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-08 15:06:02,992] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "## imports\n",
    "import os\n",
    "import deepspeed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import logging\n",
    "import json\n",
    "logging.basicConfig(filename='./logs/train-sum-qa-deepspeed.log', level=logging.INFO)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models, util, evaluation, losses, InputExample\n",
    "from TypingDataset import TypingDataset, MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data_paths for NLI\n",
    "\n",
    "data_folder_path = '/drive02/novak/datasets/downloads/qa/'\n",
    "\n",
    "data_filenames = [\n",
    "    # 'gooaq_pairs.jsonl',\n",
    "    'yahoo_answers_title_answer.jsonl',\n",
    "    'stackexchange_duplicate_questions_title_title.jsonl',\n",
    "    'eli5_question_answer.jsonl',\n",
    "    'yahoo_answers_title_question.jsonl',\n",
    "    # 'yahoo_answers_question_answer.jsonl',\n",
    "    # 'wikihow.jsonl',\n",
    "    # 'NQ-train_pairs.jsonl',\n",
    "    # 'amazon-qa-train-pairs.jsonl',\n",
    "    # 'squad_pairs.jsonl',\n",
    "    # 'quora_duplicates.jsonl',\n",
    "    # 'PAQ_pairs.jsonl',\n",
    "]\n",
    "\n",
    "def load_data(filename):\n",
    "    \n",
    "    ret = []\n",
    "    with open(f'{data_folder_path}{filename}', 'r') as rf:\n",
    "        \n",
    "        # lines = rf.readlines()\n",
    "        \n",
    "        lines = [InputExample(texts=json.loads(line)) for line in tqdm(rf)]\n",
    "        \n",
    "        return lines\n",
    "        \n",
    "        # for line in tqdm(lines):\n",
    "        #     line_data = InputExample(texts=json.loads(line))\n",
    "        #     ret.append(line_data)\n",
    "        \n",
    "    # return ret\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1198260it [00:07, 154837.78it/s]\n",
      "304525it [00:01, 199283.24it/s]\n",
      "325475it [00:01, 166403.85it/s]\n",
      "659896it [00:03, 213817.61it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for dataset in data_filenames:\n",
    "    \n",
    "    data += load_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2488156\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "pair_data_loader = DataLoader(data, batch_size=300*2, shuffle=True)\n",
    "# pair_data_loader = TypingDataLoader(data, batch_size=300*4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "djovak\n"
     ]
    }
   ],
   "source": [
    "model_name = 'all-MiniLM-L12-v2'\n",
    "print('djovak')\n",
    "# model = SentenceTransformer(model_name, device='cuda:0', )\n",
    "\n",
    "tokenizer_args = {\n",
    "  \"clean_up_tokenization_spaces\": True,\n",
    "  \"cls_token\": \"[CLS]\",\n",
    "  \"do_basic_tokenize\": True,\n",
    "  \"do_lower_case\": True,\n",
    "  \"mask_token\": \"[MASK]\",\n",
    "  \"max_length\": 128,\n",
    "  \"model_max_length\": 512,\n",
    "  \"never_split\": None,\n",
    "  \"pad_to_multiple_of\": None,\n",
    "  \"pad_token\": \"[PAD]\",\n",
    "  \"pad_token_type_id\": 0,\n",
    "  \"padding_side\": \"right\",\n",
    "  \"sep_token\": \"[SEP]\",\n",
    "  \"stride\": 0,\n",
    "  \"strip_accents\": None,\n",
    "  \"tokenize_chinese_chars\": True,\n",
    "  \"tokenizer_class\": \"BertTokenizer\",\n",
    "  \"truncation_side\": \"right\",\n",
    "  \"truncation_strategy\": \"longest_first\",\n",
    "  \"unk_token\": \"[UNK]\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_name = 'qa-yotta-L12-v1'\n",
    "word_embedding_model = models.Transformer('microsoft/MiniLM-L12-H384-uncased', max_seq_length=512, tokenizer_args=tokenizer_args)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=384, activation_function=torch.nn.Tanh())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_loss = losses.MultipleNegativesRankingLoss(model=model, similarity_fct=util.cos_sim, scale=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'LOCAL_RANK'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/novak/sbert_finetune/text_augmentation/train_qa_deepspeed.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B122/home/novak/sbert_finetune/text_augmentation/train_qa_deepspeed.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B122/home/novak/sbert_finetune/text_augmentation/train_qa_deepspeed.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(os\u001b[39m.\u001b[39;49menviron[\u001b[39m'\u001b[39;49m\u001b[39mLOCAL_RANK\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m/usr/lib/python3.8/os.py:675\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    672\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencodekey(key)]\n\u001b[1;32m    673\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m     \u001b[39m# raise KeyError with the original key value\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    676\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodevalue(value)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'LOCAL_RANK'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ['LOCAL_RANK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-08 14:55:20,692] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.12.2, git-hash=unknown, git-branch=unknown\n",
      "[2023-11-08 14:55:20,694] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-11-08 14:55:20,695] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-08 14:55:21,288] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=207.189.105.122, master_port=29500\n",
      "[2023-11-08 14:55:21,290] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2023-11-08 14:55:23,821] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "Installed CUDA version 11.2 does not match the version torch was compiled with 11.8 but since the APIs are compatible, accepting this combination\n",
      "Installed CUDA version 11.2 does not match the version torch was compiled with 11.8 but since the APIs are compatible, accepting this combination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/novak/.cache/torch_extensions/py38_cu118 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/novak/.cache/torch_extensions/py38_cu118/cpu_adam/build.ninja...\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 3.6252942085266113 seconds\n",
      "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.000200, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1\n",
      "[2023-11-08 14:55:30,333] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-08 14:55:30,335] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2023-11-08 14:55:30,344] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2023-11-08 14:55:30,344] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2023-11-08 14:55:30,345] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n",
      "[2023-11-08 14:55:30,346] [INFO] [stage_1_and_2.py:147:__init__] Reduce bucket size 200000000\n",
      "[2023-11-08 14:55:30,347] [INFO] [stage_1_and_2.py:148:__init__] Allgather bucket size 200000000\n",
      "[2023-11-08 14:55:30,347] [INFO] [stage_1_and_2.py:149:__init__] CPU Offload: True\n",
      "[2023-11-08 14:55:30,348] [INFO] [stage_1_and_2.py:150:__init__] Round robin gradient partitioning: False\n",
      "[2023-11-08 14:55:31,729] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states\n",
      "[2023-11-08 14:55:31,730] [INFO] [utils.py:803:see_memory_usage] MA 0.08 GB         Max_MA 0.08 GB         CA 0.09 GB         Max_CA 0 GB \n",
      "[2023-11-08 14:55:31,731] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 15.9 GB, percent = 12.7%\n",
      "[2023-11-08 14:55:32,757] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states\n",
      "[2023-11-08 14:55:32,759] [INFO] [utils.py:803:see_memory_usage] MA 0.08 GB         Max_MA 0.08 GB         CA 0.09 GB         Max_CA 0 GB \n",
      "[2023-11-08 14:55:32,760] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 16.23 GB, percent = 12.9%\n",
      "[2023-11-08 14:55:32,760] [INFO] [stage_1_and_2.py:514:__init__] optimizer state initialized\n",
      "[2023-11-08 14:55:33,676] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-11-08 14:55:33,678] [INFO] [utils.py:803:see_memory_usage] MA 0.08 GB         Max_MA 0.08 GB         CA 0.09 GB         Max_CA 0 GB \n",
      "[2023-11-08 14:55:33,679] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 16.23 GB, percent = 12.9%\n",
      "[2023-11-08 14:55:33,688] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
      "[2023-11-08 14:55:33,689] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\n",
      "[2023-11-08 14:55:33,689] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f5ee66ce7c0>\n",
      "[2023-11-08 14:55:33,690] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0002], mom=[(0.9, 0.999)]\n",
      "[2023-11-08 14:55:33,691] [INFO] [config.py:972:print] DeepSpeedEngine configuration:\n",
      "[2023-11-08 14:55:33,692] [INFO] [config.py:976:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-11-08 14:55:33,693] [INFO] [config.py:976:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-11-08 14:55:33,693] [INFO] [config.py:976:print]   amp_enabled .................. False\n",
      "[2023-11-08 14:55:33,694] [INFO] [config.py:976:print]   amp_params ................... False\n",
      "[2023-11-08 14:55:33,695] [INFO] [config.py:976:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-11-08 14:55:33,695] [INFO] [config.py:976:print]   bfloat16_enabled ............. False\n",
      "[2023-11-08 14:55:33,696] [INFO] [config.py:976:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-11-08 14:55:33,696] [INFO] [config.py:976:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-11-08 14:55:33,697] [INFO] [config.py:976:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-11-08 14:55:33,697] [INFO] [config.py:976:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5e01eaeca0>\n",
      "[2023-11-08 14:55:33,698] [INFO] [config.py:976:print]   communication_data_type ...... None\n",
      "[2023-11-08 14:55:33,699] [INFO] [config.py:976:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-11-08 14:55:33,699] [INFO] [config.py:976:print]   curriculum_enabled_legacy .... False\n",
      "[2023-11-08 14:55:33,700] [INFO] [config.py:976:print]   curriculum_params_legacy ..... False\n",
      "[2023-11-08 14:55:33,700] [INFO] [config.py:976:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-11-08 14:55:33,701] [INFO] [config.py:976:print]   data_efficiency_enabled ...... False\n",
      "[2023-11-08 14:55:33,701] [INFO] [config.py:976:print]   dataloader_drop_last ......... False\n",
      "[2023-11-08 14:55:33,701] [INFO] [config.py:976:print]   disable_allgather ............ False\n",
      "[2023-11-08 14:55:33,702] [INFO] [config.py:976:print]   dump_state ................... False\n",
      "[2023-11-08 14:55:33,702] [INFO] [config.py:976:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
      "[2023-11-08 14:55:33,703] [INFO] [config.py:976:print]   eigenvalue_enabled ........... False\n",
      "[2023-11-08 14:55:33,703] [INFO] [config.py:976:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-11-08 14:55:33,703] [INFO] [config.py:976:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-11-08 14:55:33,704] [INFO] [config.py:976:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-11-08 14:55:33,704] [INFO] [config.py:976:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-11-08 14:55:33,705] [INFO] [config.py:976:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-11-08 14:55:33,705] [INFO] [config.py:976:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-11-08 14:55:33,705] [INFO] [config.py:976:print]   eigenvalue_verbose ........... False\n",
      "[2023-11-08 14:55:33,706] [INFO] [config.py:976:print]   elasticity_enabled ........... False\n",
      "[2023-11-08 14:55:33,706] [INFO] [config.py:976:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-11-08 14:55:33,707] [INFO] [config.py:976:print]   fp16_auto_cast ............... False\n",
      "[2023-11-08 14:55:33,707] [INFO] [config.py:976:print]   fp16_enabled ................. auto\n",
      "[2023-11-08 14:55:33,707] [INFO] [config.py:976:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-11-08 14:55:33,708] [INFO] [config.py:976:print]   global_rank .................. 0\n",
      "[2023-11-08 14:55:33,708] [INFO] [config.py:976:print]   grad_accum_dtype ............. None\n",
      "[2023-11-08 14:55:33,709] [INFO] [config.py:976:print]   gradient_accumulation_steps .. 1\n",
      "[2023-11-08 14:55:33,709] [INFO] [config.py:976:print]   gradient_clipping ............ 1.0\n",
      "[2023-11-08 14:55:33,710] [INFO] [config.py:976:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-11-08 14:55:33,710] [INFO] [config.py:976:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-11-08 14:55:33,714] [INFO] [config.py:976:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-11-08 14:55:33,714] [INFO] [config.py:976:print]   load_universal_checkpoint .... False\n",
      "[2023-11-08 14:55:33,715] [INFO] [config.py:976:print]   loss_scale ................... 0\n",
      "[2023-11-08 14:55:33,715] [INFO] [config.py:976:print]   memory_breakdown ............. False\n",
      "[2023-11-08 14:55:33,716] [INFO] [config.py:976:print]   mics_hierarchial_params_gather  False\n",
      "[2023-11-08 14:55:33,716] [INFO] [config.py:976:print]   mics_shard_size .............. -1\n",
      "[2023-11-08 14:55:33,717] [INFO] [config.py:976:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-11-08 14:55:33,717] [INFO] [config.py:976:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-11-08 14:55:33,717] [INFO] [config.py:976:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-11-08 14:55:33,718] [INFO] [config.py:976:print]   optimizer_name ............... adamw\n",
      "[2023-11-08 14:55:33,718] [INFO] [config.py:976:print]   optimizer_params ............. {'lr': 0.0002, 'weight_decay': 0.01}\n",
      "[2023-11-08 14:55:33,718] [INFO] [config.py:976:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-11-08 14:55:33,719] [INFO] [config.py:976:print]   pld_enabled .................. False\n",
      "[2023-11-08 14:55:33,719] [INFO] [config.py:976:print]   pld_params ................... False\n",
      "[2023-11-08 14:55:33,719] [INFO] [config.py:976:print]   prescale_gradients ........... False\n",
      "[2023-11-08 14:55:33,720] [INFO] [config.py:976:print]   scheduler_name ............... WarmupLR\n",
      "[2023-11-08 14:55:33,720] [INFO] [config.py:976:print]   scheduler_params ............. {'warmup_num_steps': 200}\n",
      "[2023-11-08 14:55:33,720] [INFO] [config.py:976:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2023-11-08 14:55:33,721] [INFO] [config.py:976:print]   sparse_attention ............. None\n",
      "[2023-11-08 14:55:33,721] [INFO] [config.py:976:print]   sparse_gradients_enabled ..... False\n",
      "[2023-11-08 14:55:33,721] [INFO] [config.py:976:print]   steps_per_print .............. 10\n",
      "[2023-11-08 14:55:33,721] [INFO] [config.py:976:print]   train_batch_size ............. 300\n",
      "[2023-11-08 14:55:33,722] [INFO] [config.py:976:print]   train_micro_batch_size_per_gpu  300\n",
      "[2023-11-08 14:55:33,722] [INFO] [config.py:976:print]   use_node_local_storage ....... False\n",
      "[2023-11-08 14:55:33,722] [INFO] [config.py:976:print]   wall_clock_breakdown ......... False\n",
      "[2023-11-08 14:55:33,722] [INFO] [config.py:976:print]   weight_quantization_config ... None\n",
      "[2023-11-08 14:55:33,723] [INFO] [config.py:976:print]   world_size ................... 1\n",
      "[2023-11-08 14:55:33,723] [INFO] [config.py:976:print]   zero_allow_untested_optimizer  False\n",
      "[2023-11-08 14:55:33,723] [INFO] [config.py:976:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=True, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2023-11-08 14:55:33,724] [INFO] [config.py:976:print]   zero_enabled ................. True\n",
      "[2023-11-08 14:55:33,724] [INFO] [config.py:976:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-11-08 14:55:33,724] [INFO] [config.py:976:print]   zero_optimization_stage ...... 2\n",
      "[2023-11-08 14:55:33,725] [INFO] [config.py:962:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": \"auto\", \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"AdamW\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.0002, \n",
      "            \"weight_decay\": 0.01\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_num_steps\": 200\n",
      "        }\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\", \n",
      "            \"pin_memory\": true\n",
      "        }, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true\n",
      "    }, \n",
      "    \"comms_logger\": {\n",
      "        \"enabled\": true, \n",
      "        \"verbose\": false, \n",
      "        \"prof_all\": true, \n",
      "        \"debug\": false\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"train_micro_batch_size_per_gpu\": 300\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# print(os.environ['LOCAL_RANK'])\n",
    "\n",
    "ds_config = {\n",
    "    \"fp16\": {\n",
    "        \"enabled\": \"auto\",\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": 0.0002,\n",
    "            \"weight_decay\": 0.01\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_num_steps\": 200\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "            \"pin_memory\": True\n",
    "        },\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 2e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 2e8,\n",
    "        \"contiguous_gradients\": True\n",
    "    },\n",
    "    \n",
    "    \"comms_logger\": {\n",
    "    \"enabled\": True,\n",
    "    \"verbose\": False,\n",
    "    \"prof_all\": True,\n",
    "    \"debug\": False\n",
    "    },\n",
    "\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"train_micro_batch_size_per_gpu\": 300\n",
    "}\n",
    "\n",
    "engine, optimizer, eng_dataloader, *_ = deepspeed.initialize(\n",
    "    model=negative_loss,\n",
    "    config= ds_config,\n",
    "    training_data=data,\n",
    "    collate_fn=model.smart_batching_collate,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8294 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:SEND Error: Host unreachable\n",
      "  0%|          | 0/8294 [06:57<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from sentence_transformers.util import batch_to_device\n",
    "import os\n",
    "# os.environ['MASTER_ADDR'] = 'localhost'\n",
    "# os.environ['MASTER_PORT'] = '9994'\n",
    "# os.environ['RANK'] = \"0\"\n",
    "# os.environ['LOCAL_RANK'] = \"0\"\n",
    "# os.environ['WORLD_SIZE'] = \"1\"\n",
    "\n",
    "# if args.local_rank == -1: \n",
    "#     args.local_rank = 0\n",
    "\n",
    "# 3. finally init deepspeed dist and set the default device\n",
    "# deepspeed.init_distributed()\n",
    "# device = torch.device(\"cuda\", 0)\n",
    "\n",
    "# pair_data_loader.collate_fn = model.smart_batching_collate\n",
    "\n",
    "for batch in tqdm(eng_dataloader):\n",
    "    features, labels = batch\n",
    "    # labels.to(model._target_device)\n",
    "    # features = list(map(lambda batch: batch_to_device(batch, model._target_device), features))\n",
    "    loss = engine(features, labels)\n",
    "    engine.backward(loss)\n",
    "    engine.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     model.fit(train_objectives=[(pair_data_loader, negative_loss)], \n",
    "#             epochs=10,\n",
    "#             use_amp=True,\n",
    "#             warmup_steps = int(len(pair_data_loader)*0.1), \n",
    "#             weight_decay = 0.01,\n",
    "#             evaluation_steps=100, \n",
    "#             checkpoint_save_total_limit=20,\n",
    "#             checkpoint_save_steps=100,\n",
    "#             checkpoint_path = f'./typing-checkpoints/qa-yotta-{model_name}/',\n",
    "#             output_path=f'./typing/qa-yotta-{model_name}/')\n",
    "# finally:\n",
    "#     import torch\n",
    "#     with torch.no_grad():\n",
    "#         torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "\n",
    "# vizulizacija loss-a na weights and biases\n",
    "# pozivanje MTAB benchmark-a kao eval\n",
    "# prelazak pipeline-a na Deepspeed(fp16, optimizovanija paralelizacija)\n",
    "# vizulatizacija EVAL MTAB metrika, kako rastu tokom treninga\n",
    "# \n",
    "\n",
    "# optimizacija citanja podataka, da ne bude sve u ramu\n",
    "# MOZDA skupi jos podataka\n",
    "# MOZDA KVALITET uzimanje iz dataset-a po pravilu iz papira\n",
    "# MOZDA(low priority) MTAB PARALLEL GPU\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
